var documenterSearchIndex = {"docs":
[{"location":"devdocs/constructing_loopsets/#Constructing-LoopSets-1","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/#Loop-expressions-1","page":"Constructing LoopSets","title":"Loop expressions","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When applying @avx to a loop expression, it creates a LoopSet without awareness to type information, and then condenses the information into a summary which is passed as type information to a generated function.","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"julia> @macroexpand @avx for m ∈ 1:M, n ∈ 1:N\n           C[m,n] = zero(eltype(B))\n           for k ∈ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end\nquote\n    var\"##vptr##_C\" = LoopVectorization.stridedpointer(C)\n    var\"##vptr##_A\" = LoopVectorization.stridedpointer(A)\n    var\"##vptr##_B\" = LoopVectorization.stridedpointer(B)\n    begin\n        $(Expr(:gc_preserve, :(LoopVectorization._avx_!(Val{(0, 0)}(), Tuple{:numericconstant, Symbol(\"##zero#270\"), LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.constant, 0x00, 0x01), :LoopVectorization, :setindex!, LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000000, 0x0000000000000007, LoopVectorization.memstore, 0x01, 0x02), :LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000013, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x02, 0x03), :LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000032, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x03, 0x04), :numericconstant, Symbol(\"##reductzero#274\"), LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000000, 0x0000000000000003, 0x0000000000000000, LoopVectorization.constant, 0x00, 0x05), :LoopVectorization, :vfmadd_fast, LoopVectorization.OperationStruct(0x0000000000000132, 0x0000000000000003, 0x0000000000000000, 0x0000000000030405, LoopVectorization.compute, 0x00, 0x05), :LoopVectorization, :reduce_to_add, LoopVectorization.OperationStruct(0x0000000000000012, 0x0000000000000003, 0x0000000000000000, 0x0000000000000601, LoopVectorization.compute, 0x00, 0x01)}, Tuple{LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffe03b), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000103, 0xffffffffffffffd6), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000302, 0xffffffffffffe056), LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffffd6)}, Tuple{0, Tuple{}, Tuple{}, Tuple{}, Tuple{}, Tuple{(1, LoopVectorization.IntOrFloat), (5, LoopVectorization.IntOrFloat)}, Tuple{}}, (LoopVectorization.StaticLowerUnitRange{0}(M), LoopVectorization.StaticLowerUnitRange{0}(N), LoopVectorization.StaticLowerUnitRange{0}(K)), var\"##vptr##_C\", var\"##vptr##_A\", var\"##vptr##_B\", var\"##vptr##_C\")), :C, :A, :B))\n    end\nend","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When the corresponding method gets compiled for specific type of A, B, and C, the call to the @generated function _avx_! get compiled. This causes the summary to be reconstructed using the available type information. This type information can be used, for example, to realize an array has been transposed, and thus correctly identify which axis contains contiguous elements that are efficient to load from. This kind of information cannot be extracted from the raw expression, which is why these decisions are made when the method gets compiled for specific types via the @generated function _avx_!.","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"The three chief components of the summaries are the definitions of operations, e.g.:","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":":LoopVectorization, :getindex, LoopVectorization.OperationStruct(0x0000000000000013, 0x0000000000000000, 0x0000000000000000, 0x0000000000000000, LoopVectorization.memload, 0x02, 0x03)","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"the referenced array objects:","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"LoopVectorization.ArrayRefStruct(0x0000000000000101, 0x0000000000000102, 0xffffffffffffe03b)","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"and the set of loop bounds:","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"(LoopVectorization.StaticLowerUnitRange{0}(M), LoopVectorization.StaticLowerUnitRange{0}(N), LoopVectorization.StaticLowerUnitRange{0}(K))","category":"page"},{"location":"devdocs/constructing_loopsets/#Broadcasting-1","page":"Constructing LoopSets","title":"Broadcasting","text":"","category":"section"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"When applying the @avx macro to a broadcast expression, there are no explicit loops, and even the dimensionality of the operation is unknown.  Consequently the LoopSet object must be constructed at compile time. The function and involved operations are their relationships are straightforward to infer from the structure of nested broadcasts:","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"julia> Meta.@lower @. f(g(a,b) + c) / d\n:($(Expr(:thunk, CodeInfo(\n    @ none within `top-level scope'\n1 ─ %1 = Base.broadcasted(g, a, b)\n│   %2 = Base.broadcasted(+, %1, c)\n│   %3 = Base.broadcasted(f, %2)\n│   %4 = Base.broadcasted(/, %3, d)\n│   %5 = Base.materialize(%4)\n└──      return %5\n))))\n\njulia> @macroexpand @avx @. f(g(a,b) + c) / d\nquote\n    var\"##262\" = Base.broadcasted(g, a, b)\n    var\"##263\" = Base.broadcasted(+, var\"##262\", c)\n    var\"##264\" = Base.broadcasted(f, var\"##263\")\n    var\"##265\" = Base.broadcasted(/, var\"##264\", d)\n    var\"##266\" = LoopVectorization.vmaterialize(var\"##265\", Val{:Main}())\nend","category":"page"},{"location":"devdocs/constructing_loopsets/#","page":"Constructing LoopSets","title":"Constructing LoopSets","text":"These nested broadcasted objects already express information very similar to what the LoopSet objects hold. The dimensionality of the objects provides the information on the associated loop dependencies, but again this information is available only when the method is compiled for specific types. The @generated function vmaterialize constructs the LoopSet by recursively evaluating add_broadcast! on all the fields.","category":"page"},{"location":"examples/matrix_vector_ops/#Matrix-Vector-Operations-1","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"","category":"section"},{"location":"examples/matrix_vector_ops/#","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Here I'll discuss a variety of Matrix-vector operations, naturally starting with matrix-vector multiplication.","category":"page"},{"location":"examples/matrix_vector_ops/#","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"function jgemvavx!(𝐲, 𝐀, 𝐱)\n    @avx for i ∈ eachindex(𝐲)\n        𝐲ᵢ = zero(eltype(𝐲))\n        for j ∈ eachindex(𝐱)\n            𝐲ᵢ += 𝐀[i,j] * 𝐱[j]\n        end\n        𝐲[i] = 𝐲ᵢ\n    end\nend","category":"page"},{"location":"examples/matrix_vector_ops/#","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Using a square Size x Size matrix 𝐀, we find the following results. (Image: Amulvb)","category":"page"},{"location":"examples/matrix_vector_ops/#","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"If 𝐀 is transposed, or equivalently, if we're instead computing x * 𝐀: (Image: Atmulvb)","category":"page"},{"location":"examples/matrix_vector_ops/#","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"Finally, the three-argument dot product y' * 𝐀 * x: (Image: dot3)","category":"page"},{"location":"examples/matrix_vector_ops/#","page":"Matrix-Vector Operations","title":"Matrix-Vector Operations","text":"The performance impact of alignment is dramatic here.","category":"page"},{"location":"devdocs/lowering/#Lowering-1","page":"Lowering","title":"Lowering","text":"","category":"section"},{"location":"devdocs/lowering/#","page":"Lowering","title":"Lowering","text":"The first step to lowering is picking a strategy for lowering the loops. Then a Julia expression is created following that strategy, converting each of the operations into Julia expressions. This task is made simpler via multiple dispatch making the lowering of the components independent of the larger picture. For example, a load will look like","category":"page"},{"location":"devdocs/lowering/#","page":"Lowering","title":"Lowering","text":"vload(vptr_A, (i,j,k))","category":"page"},{"location":"devdocs/lowering/#","page":"Lowering","title":"Lowering","text":"with the behavior of this load determined by the types of the arguments. Vectorization is expressed by making an index a _MM{W} type, rather than an integer, and operations with it will either produce another _MM{W} when it will still correspond to contiguous loads, or an SVec{W,<:Integer} if the resulting loads will be discontiguous, so that a gather or scatter! will be used. If all indexes are simply integers, then this produces a scalar load or store.","category":"page"},{"location":"examples/special_functions/#Special-Functions-1","page":"Special Functions","title":"Special Functions","text":"","category":"section"},{"location":"examples/special_functions/#","page":"Special Functions","title":"Special Functions","text":"LoopVectorization supports vectorizing many special functions, for example, to calculate the log determinant of a triangular matrix:","category":"page"},{"location":"examples/special_functions/#","page":"Special Functions","title":"Special Functions","text":"function logdettriangle(B::Union{LowerTriangular,UpperTriangular})\n    A = parent(B) # using a triangular matrix would fall back to the default loop.\n    ld = zero(eltype(A))\n    @avx for n ∈ axes(A,1)\n        ld += log(A[n,n])\n    end\n    ld\nend","category":"page"},{"location":"examples/special_functions/#","page":"Special Functions","title":"Special Functions","text":"(Image: selfdot)","category":"page"},{"location":"examples/special_functions/#","page":"Special Functions","title":"Special Functions","text":"While Intel's proprietary compilers do the best, LoopVectorization performs very well among open source alternatives. A complicating factor to the above benchmark is that in accessing the diagonals, we are not accessing contiguous elements. A benchmark simply exponentiating a vector shows that gcc also has efficient special function vectorization, but that the autovectorizer disagrees with the discontiguous memory acesses:","category":"page"},{"location":"examples/special_functions/#","page":"Special Functions","title":"Special Functions","text":"(Image: selfdot)","category":"page"},{"location":"examples/special_functions/#","page":"Special Functions","title":"Special Functions","text":"The similar performance between gfortran and LoopVectorization at multiples of 8 is no fluke: on Linux systems with a recent GLIBC, SLEEFPirates.jl – which LoopVectorization depends on to vectorize these special functions – looks for the GNU vector library and uses these functions if available. Otherwise, it will use native Julia implementations that tend to be slower. As the modulus of vector length and vector width (8, on the host system thanks to AVX512) increases, gfortran shows the performance degredation pattern typical of LLVM-vectorized code.","category":"page"},{"location":"devdocs/evaluating_loops/#Determining-the-strategy-for-evaluating-loops-1","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"","category":"section"},{"location":"devdocs/evaluating_loops/#","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The heart of the optimizatizations performed by LoopVectorization are given in the determinestrategy.jl file utilizing instruction costs specified in costs.jl. Essentially, it estimates the cost of different means of evaluating the loops. It iterates through the different possible loop orders, as well as considering which loops to unroll, and which to vectorize. It will consider unrolling 1 or 2 loops (but it could settle on unrolling by a factor of 1, i.e. not unrolling), and vectorizing 1.","category":"page"},{"location":"devdocs/evaluating_loops/#","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The cost estimate is based on the costs of individual instructions and the number of times each one needs to be executed for the given strategy. The instruction cost can be broken into several components:","category":"page"},{"location":"devdocs/evaluating_loops/#","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"The scalar latency is the minimum delay, in clock cycles, associated with the instruction. Think of it as the delay from turning on the water to when water starts coming out the hose.\nThe reciprocal throughput is similar to the latency, but it measures the number of cycles per operation when many of the same operation are repeated in sequence.  Continuing our hose analogy, think of it as the inverse of the flow rate at steady-state. It is typically ≤ the scalar latency.\nThe register pressure measures the register consumption by the operation","category":"page"},{"location":"devdocs/evaluating_loops/#","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Data on individual instructions for specific architectures can be found on Agner Fog's website. Most of the costs used were those for the Skylake-X architecture.","category":"page"},{"location":"devdocs/evaluating_loops/#","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Examples of how these come into play:","category":"page"},{"location":"devdocs/evaluating_loops/#","page":"Determining the strategy for evaluating loops","title":"Determining the strategy for evaluating loops","text":"Vectorizing a loop will result in each instruction evaluating multiple iterations, but the costs of loads and stores will change based on the memory layouts of the accessed arrays.\nUnrolling can help reduce the number of times an operation must be performed, for example if it can allow us to reuse memory multiple times rather than reloading it every time it is needed.\nWhen there is a reduction, such as performing a sum, there is a dependency chain. Each + has to wait for the previous + to finish executing before it can begin, thus execution time is bounded by latency rather than minimum of the throughput of the + and load operations. By unrolling the loop, we can create multiple independent dependency chains.","category":"page"},{"location":"future_work/#Future-Plans-1","page":"Future Work","title":"Future Plans","text":"","category":"section"},{"location":"future_work/#","page":"Future Work","title":"Future Work","text":"Future plans for LoopVectorization:","category":"page"},{"location":"future_work/#","page":"Future Work","title":"Future Work","text":"Support triangular iteration spaces.\nIdentify obvious loop-carried dependencies like A[j] and A[j-1].\nBe able to generate optimized kernels from simple loop-based implementations of operations like Cholesky decompositions or solving triangular systems of equations.\nModel memory and CPU-cache to possibly insert extra loops and packing of data when deemed profitable.\nTrack types of individual operations in the loops. Currently, multiple types in loops aren't really handled, so this is a bit brittle at the moment.\nHandle loops where arrays contain non-primitive types (e.g., Complex numbers) well.","category":"page"},{"location":"future_work/#","page":"Future Work","title":"Future Work","text":"Contributions are more than welcome, and I would be happy to assist if anyone would like to take a stab at any of these. Otherwise, while LoopVectorization is a core component to much of my work, so that I will continue developing it, I have many other projects that require active development, so it will be a long time before I am able to address these myself.","category":"page"},{"location":"devdocs/overview/#Developer-Overview-1","page":"Developer Overview","title":"Developer Overview","text":"","category":"section"},{"location":"devdocs/overview/#","page":"Developer Overview","title":"Developer Overview","text":"Here I will try to explain how the library works for the curious or any would-be contributors.","category":"page"},{"location":"devdocs/overview/#","page":"Developer Overview","title":"Developer Overview","text":"The library uses a LoopSet object to model loops. The key components of the library can be divided into:","category":"page"},{"location":"devdocs/overview/#","page":"Developer Overview","title":"Developer Overview","text":"Defining the LoopSet objects.\nConstructing the LoopSet objects.\nDetermining the strategy of how to evaluate loops.\nLowering the loopset object into a Julia Expr following a strategy.","category":"page"},{"location":"api/#API-reference-1","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/#Macros-1","page":"API reference","title":"Macros","text":"","category":"section"},{"location":"api/#","page":"API reference","title":"API reference","text":"@avx\n@_avx","category":"page"},{"location":"api/#LoopVectorization.@avx","page":"API reference","title":"LoopVectorization.@avx","text":"@avx\n\nAnnotate a for loop, or a set of nested for loops whose bounds are constant across iterations, to optimize the computation. For example:\n\nfunction AmulBavx!(C, A, B)\n    @avx for m ∈ 1:size(A,1), n ∈ 1:size(B,2)\n        Cₘₙ = zero(eltype(C))\n        for k ∈ 1:size(A,2)\n            Cₘₙ += A[m,k] * B[k,n]\n        end\n        C[m,n] = Cₘₙ\n    end\nend\n\nThe macro models the set of nested loops, and chooses an ordering of the three loops to minimize predicted computation time.\n\nIt may also apply to broadcasts:\n\njulia> using LoopVectorization\n\njulia> a = rand(100);\n\njulia> b = @avx exp.(2 .* a);\n\njulia> c = similar(b);\n\njulia> @avx @. c = exp(2a);\n\njulia> b ≈ c\ntrue\n\nExtended help\n\nAdvanced users can customize the implementation of the @avx-annotated block using keyword arguments:\n\n@avx inline=false unroll=2 body\n\nwhere body is the code of the block (e.g., for ... end).\n\ninline is a Boolean. When true, body will be directly inlined into the function (via a forced-inlining call to _avx_!). When false, it wont force inlining of the call to _avx_! instead, letting Julia's own inlining engine determine whether the call to _avx_! should be inlined. (Typically, it won't.) Sometimes not inlining can lead to substantially worse code generation, and >40% regressions, even in very large problems (2-d convolutions are a case where this has been observed). One can find some circumstances where inline=true is faster, and other circumstances where inline=false is faster, so the best setting may require experimentation. By default, the macro tries to guess. Currently the algorithm is simple: roughly, if there are more than two dynamically sized loops or and no convolutions, it will probably not force inlining. Otherwise, it probably will.\n\ncheck_empty (default is false) determines whether or not it will check if any of the iterators are empty. If false, you must ensure yourself that they are not empty, else the behavior of the loop is undefined and (like with @inbounds) segmentation faults are likely.\n\nunroll is an integer that specifies the loop unrolling factor, or a tuple (u₁, u₂) = (4, 2) signaling that the generated code should unroll more than one loop. u₁ is the unrolling factor for the first unrolled loop and u₂ for the next (if present), but it applies to the loop ordering and unrolling that will be chosen by LoopVectorization, not the order in body. uᵢ=0 (the default) indicates that LoopVectorization should pick its own value, and uᵢ=-1 disables unrolling for the correspond loop.\n\nThe @avx macro also checks the array arguments using LoopVectorization.check_args to try and determine if they are compatible with the macro. If check_args returns false, a fall back loop annotated with @inbounds and @fastmath is generated. Note that SIMDPirates provides functions such as evadd and evmul that will ignore @fastmath, preserving IEEE semantics both within @avx and @fastmath. check_args currently returns false for some wrapper types like LinearAlgebra.UpperTriangular, requiring you to use their parent. Triangular loops aren't yet supported.\n\n\n\n\n\n","category":"macro"},{"location":"api/#LoopVectorization.@_avx","page":"API reference","title":"LoopVectorization.@_avx","text":"@_avx\n\nThis macro transforms loops similarly to @avx. While @avx punts to a generated function to enable type-based analysis, _@avx works on just the expressions. This requires that it makes a number of default assumptions. Use of @avx is preferred.\n\nThis macro accepts the inline and unroll keyword arguments like @avx, but ignores the check_empty argument.\n\n\n\n\n\n","category":"macro"},{"location":"api/#map-like-constructs-1","page":"API reference","title":"map-like constructs","text":"","category":"section"},{"location":"api/#","page":"API reference","title":"API reference","text":"vmap\nvmap!\nvmapnt\nvmapnt!\nvmapntt\nvmapntt!","category":"page"},{"location":"api/#LoopVectorization.vmap","page":"API reference","title":"LoopVectorization.vmap","text":"vmap(f, a::AbstractArray)\nvmap(f, a::AbstractArray, b::AbstractArray, ...)\n\nSIMD-vectorized map, applying f to each element of a (or paired elements of a, b, ...) and returning a new array.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmap!","page":"API reference","title":"LoopVectorization.vmap!","text":"vmap!(f, destination, a::AbstractArray)\nvmap!(f, destination, a::AbstractArray, b::AbstractArray, ...)\n\nVectorized-map!, applying f to each element of a (or paired elements of a, b, ...) and storing the result in destination.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapnt","page":"API reference","title":"LoopVectorization.vmapnt","text":"vmapnt(f, a::AbstractArray)\nvmapnt(f, a::AbstractArray, b::AbstractArray, ...)\n\nA \"non-temporal\" variant of vmap. This can improve performance in cases where destination will not be needed soon.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapnt!","page":"API reference","title":"LoopVectorization.vmapnt!","text":"vmapnt!(::Function, dest, args...)\n\nThis is a vectorized map implementation using nontemporal store operations. This means that the write operations to the destination will not go to the CPU's cache. If you will not immediately be reading from these values, this can improve performance because the writes won't pollute your cache. This can especially be the case if your arguments are very long.\n\njulia> using LoopVectorization, BenchmarkTools\n\njulia> x = rand(10^8); y = rand(10^8); z = similar(x);\n\njulia> f(x,y) = exp(-0.5abs2(x - y))\nf (generic function with 1 method)\n\njulia> @benchmark map!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     439.613 ms (0.00% GC)\n  median time:      440.729 ms (0.00% GC)\n  mean time:        440.695 ms (0.00% GC)\n  maximum time:     441.665 ms (0.00% GC)\n  --------------\n  samples:          12\n  evals/sample:     1\n\njulia> @benchmark vmap!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     178.147 ms (0.00% GC)\n  median time:      178.381 ms (0.00% GC)\n  mean time:        178.430 ms (0.00% GC)\n  maximum time:     179.054 ms (0.00% GC)\n  --------------\n  samples:          29\n  evals/sample:     1\n\njulia> @benchmark vmapnt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     144.183 ms (0.00% GC)\n  median time:      144.338 ms (0.00% GC)\n  mean time:        144.349 ms (0.00% GC)\n  maximum time:     144.641 ms (0.00% GC)\n  --------------\n  samples:          35\n  evals/sample:     1\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapntt","page":"API reference","title":"LoopVectorization.vmapntt","text":"vmapntt(f, a::AbstractArray)\nvmapntt(f, a::AbstractArray, b::AbstractArray, ...)\n\nA threaded variant of vmapnt.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vmapntt!","page":"API reference","title":"LoopVectorization.vmapntt!","text":"vmapntt!(::Function, dest, args...)\n\nLike vmapnt! (see vmapnt!), but uses Threads.@threads for parallel execution.\n\n\n\n\n\n","category":"function"},{"location":"api/#filter-like-constructs-1","page":"API reference","title":"filter-like constructs","text":"","category":"section"},{"location":"api/#","page":"API reference","title":"API reference","text":"vfilter\nLoopVectorization.vfilter!","category":"page"},{"location":"api/#LoopVectorization.vfilter","page":"API reference","title":"LoopVectorization.vfilter","text":"vfilter(f, a::AbstractArray)\n\nSIMD-vectorized filter, returning an array containing the elements of a for which f return true.\n\n\n\n\n\n","category":"function"},{"location":"api/#LoopVectorization.vfilter!","page":"API reference","title":"LoopVectorization.vfilter!","text":"vfilter!(f, a::AbstractArray)\n\nSIMD-vectorized filter!, removing the element of a for which f is false.\n\n\n\n\n\n","category":"function"},{"location":"devdocs/reference/#Internals-reference-1","page":"Internals reference","title":"Internals reference","text":"","category":"section"},{"location":"devdocs/reference/#Operation-types-1","page":"Internals reference","title":"Operation types","text":"","category":"section"},{"location":"devdocs/reference/#","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.OperationType\nLoopVectorization.constant\nLoopVectorization.memload\nLoopVectorization.compute\nLoopVectorization.memstore\nLoopVectorization.loopvalue","category":"page"},{"location":"devdocs/reference/#LoopVectorization.OperationType","page":"Internals reference","title":"LoopVectorization.OperationType","text":"OperationType is an @enum for classifying supported operations that can appear in @avx blocks. Type LoopVectorization.OperationType to see the different types.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.constant","page":"Internals reference","title":"LoopVectorization.constant","text":"An operation setting a variable to a constant value (e.g., a = 0.0)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.memload","page":"Internals reference","title":"LoopVectorization.memload","text":"An operation setting a variable from a memory location (e.g., a = A[i,j])\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.compute","page":"Internals reference","title":"LoopVectorization.compute","text":"An operation computing a new value from one or more variables (e.g., a = b + c)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.memstore","page":"Internals reference","title":"LoopVectorization.memstore","text":"An operation storing a value to a memory location (e.g., A[i,j] = a)\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#LoopVectorization.loopvalue","page":"Internals reference","title":"LoopVectorization.loopvalue","text":"loopvalue indicates an loop variable (i in for i in ...). These are the \"parents\" of compute operations that involve the loop variables.\n\n\n\n\n\n","category":"constant"},{"location":"devdocs/reference/#Operation-1","page":"Internals reference","title":"Operation","text":"","category":"section"},{"location":"devdocs/reference/#","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.Operation","category":"page"},{"location":"devdocs/reference/#LoopVectorization.Operation","page":"Internals reference","title":"LoopVectorization.Operation","text":"Operation\n\nA structure to encode a particular action occuring inside an @avx block.\n\nFields\n\nidentifier::Int64\nA unique identifier for this operation. identifer(op::Operation) returns the index of this operation within operations(ls::LoopSet).\nvariable::Symbol\nThe name of the variable storing the result of this operation. For a = val this would be :a. For array assignments A[i,j] = val this would be :A.\nelementbytes::Int64\nIntended to be the size of the result, in bytes. Often inaccurate, not to be relied on.\ninstruction::LoopVectorization.Instruction\nThe specific operator, e.g., identity or +\nnode_type::LoopVectorization.OperationType\nThe OperationType associated with this operation\ndependencies::Array{Symbol,1}\nThe loop variables this operation depends on\nreduced_deps::Array{Symbol,1}\nAdditional loop dependencies that must execute before this operation can be performed successfully (often needed in reductions)\nparents::Array{LoopVectorization.Operation,1}\nOperations whose result this operation depends on\nref::LoopVectorization.ArrayReferenceMeta\nFor memload or memstore, encodes the array location\nmangledvariable::Symbol\ngensymmed name of result.\nreduced_children::Array{Symbol,1}\nLoop variables that consumers of this operation depend on. Often used in reductions to replicate assignment of initializers when unrolling.\nu₁unrolled::Bool\nCached value for whether u₁loopsym ∈ loopdependencies(op)\nu₂unrolled::Bool\nCached value for whether u₂loopsym ∈ loopdependencies(op)\nvectorized::Bool\nCached value for whether vectorized ∈ loopdependencies(op)\n\nExample\n\njulia> using LoopVectorization\n\njulia> AmulBq = :(for m ∈ 1:M, n ∈ 1:N\n           C[m,n] = zero(eltype(B))\n           for k ∈ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end);\n\njulia> lsAmulB = LoopVectorization.LoopSet(AmulBq);\n\njulia> LoopVectorization.operations(lsAmulB)\n6-element Array{LoopVectorization.Operation,1}:\n var\"##RHS#253\" = var\"##zero#254\"\n C[m, n] = var\"##RHS#253\"\n var\"##tempload#255\" = A[m, k]\n var\"##tempload#256\" = B[k, n]\n var\"##RHS#253\" = LoopVectorization.vfmadd_fast(var\"##tempload#255\", var\"##tempload#256\", var\"##RHS#253\")\n var\"##RHS#253\" = LoopVectorization.identity(var\"##RHS#253\")\n\nEach one of these lines is a pretty-printed Operation.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Instructions-and-costs-1","page":"Internals reference","title":"Instructions and costs","text":"","category":"section"},{"location":"devdocs/reference/#","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.Instruction\nLoopVectorization.InstructionCost","category":"page"},{"location":"devdocs/reference/#LoopVectorization.Instruction","page":"Internals reference","title":"LoopVectorization.Instruction","text":"Instruction\n\nInstruction represents a function via its module and symbol. It is similar to a GlobalRef and may someday be replaced by GlobalRef.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.InstructionCost","page":"Internals reference","title":"LoopVectorization.InstructionCost","text":"InstructionCost\n\nStore parameters related to performance for individual CPU instructions.\n\nscaling::Float64\nA flag indicating how instruction cost scales with vector width (128, 256, or 512 bits)\nscalar_reciprocal_throughput::Float64\nThe number of clock cycles per operation when many of the same operation are repeated in sequence. Think of it as the inverse of the flow rate at steady-state. It is typically ≤ the scalar_latency.\nscalar_latency::Int64\nThe minimum delay, in clock cycles, associated with the instruction. Think of it as the delay from turning on a faucet to when water starts coming out the end of the pipe. See also scalar_reciprocal_throughput.\nregister_pressure::Int64\nNumber of floating-point registered used\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Array-references-1","page":"Internals reference","title":"Array references","text":"","category":"section"},{"location":"devdocs/reference/#","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.ArrayReference\nLoopVectorization.ArrayReferenceMeta","category":"page"},{"location":"devdocs/reference/#LoopVectorization.ArrayReference","page":"Internals reference","title":"LoopVectorization.ArrayReference","text":"ArrayReference\n\nA type for encoding an array reference A[i,j] occurring inside an @avx block.\n\nFields\n\narray::Symbol\nThe array variable\nindices::Array{Symbol,1}\nThe list of indices (e.g., [:i, :j]), or name(op) for computed indices.\noffsets::Array{Int8,1}\nIndex offset, e.g., a[i+7] would store the 7. offsets is also used to help identify opportunities for avoiding reloads, for example in y[i] = x[i] - x[i-1], the previous load x[i-1] can be \"carried over\" to the next iteration. Only used for small (Int8) offsets.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.ArrayReferenceMeta","page":"Internals reference","title":"LoopVectorization.ArrayReferenceMeta","text":"ArrayReferenceMeta\n\nA type similar to ArrayReference but holding additional information.\n\nFields\n\nref::LoopVectorization.ArrayReference\nThe ArrayReference\nloopedindex::Array{Bool,1}\nA vector of Bools indicating whether each index is a loop variable (false for operation-computed indices)\nptr::Symbol\nVariable holding the pointer to the array's underlying storage\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#Condensed-types-1","page":"Internals reference","title":"Condensed types","text":"","category":"section"},{"location":"devdocs/reference/#","page":"Internals reference","title":"Internals reference","text":"These are used when encoding the @avx block as a type parameter for passing through to the @generated function.","category":"page"},{"location":"devdocs/reference/#","page":"Internals reference","title":"Internals reference","text":"LoopVectorization.ArrayRefStruct\nLoopVectorization.OperationStruct","category":"page"},{"location":"devdocs/reference/#LoopVectorization.ArrayRefStruct","page":"Internals reference","title":"LoopVectorization.ArrayRefStruct","text":"ArrayRefStruct\n\nA condensed representation of an ArrayReference. It supports array-references with up to 8 indexes, where the data for each consecutive index is packed into corresponding 8-bit fields of index_types (storing the enum IndexType), indices (the id for each index symbol), and offsets (currently unused).\n\n\n\n\n\n","category":"type"},{"location":"devdocs/reference/#LoopVectorization.OperationStruct","page":"Internals reference","title":"LoopVectorization.OperationStruct","text":"OperationStruct\n\nA condensed representation of an Operation.\n\n\n\n\n\n","category":"type"},{"location":"vectorized_convenience_functions/#Convenient-Vectorized-Functions-1","page":"Vectorized Convenience Functions","title":"Convenient Vectorized Functions","text":"","category":"section"},{"location":"vectorized_convenience_functions/#vmap-1","page":"Vectorized Convenience Functions","title":"vmap","text":"","category":"section"},{"location":"vectorized_convenience_functions/#","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"This is simply a vectorized map function.","category":"page"},{"location":"vectorized_convenience_functions/#vmapnt-and-vmapntt-1","page":"Vectorized Convenience Functions","title":"vmapnt and vmapntt","text":"","category":"section"},{"location":"vectorized_convenience_functions/#","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"These are like vmap, but use non-temporal (streaming) stores into the destination, to avoid polluting the cache. Likely to yield a performance increase if you wont be reading the values soon.","category":"page"},{"location":"vectorized_convenience_functions/#","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> f(x,y) = exp(-0.5abs2(x - y))\nf (generic function with 1 method)\n\njulia> x = rand(10^8); y = rand(10^8); z = similar(x);\n\njulia> @benchmark map!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     442.614 ms (0.00% GC)\n  median time:      443.750 ms (0.00% GC)\n  mean time:        443.664 ms (0.00% GC)\n  maximum time:     444.730 ms (0.00% GC)\n  --------------\n  samples:          12\n  evals/sample:     1\n\njulia> @benchmark vmap!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     177.257 ms (0.00% GC)\n  median time:      177.380 ms (0.00% GC)\n  mean time:        177.423 ms (0.00% GC)\n  maximum time:     177.956 ms (0.00% GC)\n  --------------\n  samples:          29\n  evals/sample:     1\n\njulia> @benchmark vmapnt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     143.521 ms (0.00% GC)\n  median time:      143.639 ms (0.00% GC)\n  mean time:        143.645 ms (0.00% GC)\n  maximum time:     143.821 ms (0.00% GC)\n  --------------\n  samples:          35\n  evals/sample:     1\n\njulia> Threads.nthreads()\n36\n\njulia> @benchmark vmapntt!(f, $z, $x, $y)\nBenchmarkTools.Trial:\n  memory estimate:  25.69 KiB\n  allocs estimate:  183\n  --------------\n  minimum time:     30.065 ms (0.00% GC)\n  median time:      30.130 ms (0.00% GC)\n  mean time:        30.146 ms (0.00% GC)\n  maximum time:     31.277 ms (0.00% GC)\n  --------------\n  samples:          166\n  evals/sample:     1","category":"page"},{"location":"vectorized_convenience_functions/#vfilter-1","page":"Vectorized Convenience Functions","title":"vfilter","text":"","category":"section"},{"location":"vectorized_convenience_functions/#","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"This function requires LLVM 7 or greater, and is only likly to give better performance if your CPU has AVX512. This is because it uses the compressed store intrinsic, which was added in LLVM 7. AVX512 provides a corresponding instruction, making the operation fast, while other instruction sets must emulate it, and thus are likely to get similar performance with LoopVectorization.vfilter as they do from Base.filter.","category":"page"},{"location":"vectorized_convenience_functions/#","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> x = rand(997);\n\njulia> y1 = filter(a -> a > 0.7, x);\n\njulia> y2 = vfilter(a -> a > 0.7, x);\n\njulia> y1 == y2\ntrue\n\njulia> @benchmark filter(a -> a > 0.7, $x)\nBenchmarkTools.Trial:\n  memory estimate:  7.94 KiB\n  allocs estimate:  1\n  --------------\n  minimum time:     955.389 ns (0.00% GC)\n  median time:      1.050 μs (0.00% GC)\n  mean time:        1.191 μs (9.72% GC)\n  maximum time:     82.799 μs (94.92% GC)\n  --------------\n  samples:          10000\n  evals/sample:     18\n\njulia> @benchmark vfilter(a -> a > 0.7, $x)\nBenchmarkTools.Trial:\n  memory estimate:  7.94 KiB\n  allocs estimate:  1\n  --------------\n  minimum time:     477.487 ns (0.00% GC)\n  median time:      575.166 ns (0.00% GC)\n  mean time:        711.526 ns (17.87% GC)\n  maximum time:     9.257 μs (79.17% GC)\n  --------------\n  samples:          10000\n  evals/sample:     193","category":"page"},{"location":"vectorized_convenience_functions/#vmapreduce-1","page":"Vectorized Convenience Functions","title":"vmapreduce","text":"","category":"section"},{"location":"vectorized_convenience_functions/#","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"Vectorized version of mapreduce. vmapreduce(f, op, a, b, c) applies f(a[i], b[i], c[i]) for i in eachindex(a,b,c), reducing the results to a scalar with op.","category":"page"},{"location":"vectorized_convenience_functions/#","page":"Vectorized Convenience Functions","title":"Vectorized Convenience Functions","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> x = rand(127); y = rand(127);\n\njulia> @btime vmapreduce(hypot, +, $x, $y)\n  191.420 ns (0 allocations: 0 bytes)\n96.75538300513509\n\njulia> @btime mapreduce(hypot, +, $x, $y)\n  1.777 μs (5 allocations: 1.25 KiB)\n96.75538300513509","category":"page"},{"location":"examples/matrix_multiplication/#Matrix-Multiplication-1","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"One of the friendliest problems for vectorization is matrix multiplication. Given M × K matrix 𝐀, and K × N matrix 𝐁, multiplying them is like performing M * N dot products of length K. We need M*K + K*N + M*N total memory, but M*K*N multiplications and additions, so there's a lot more arithmetic we can do relative to the memory needed.","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"LoopVectorization currently doesn't do any memory-modeling or memory-based optimizations, so it will still run into problems as the size of matrices increases. But at smaller sizes, it's capable of achieving a healthy percent of potential GFLOPS. We can write a single function:","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"function A_mul_B!(𝐂, 𝐀, 𝐁)\n    @avx for m ∈ axes(𝐀,1), n ∈ axes(𝐁,2)\n        𝐂ₘₙ = zero(eltype(𝐂))\n        for k ∈ axes(𝐀,2)\n            𝐂ₘₙ += 𝐀[m,k] * 𝐁[k,n]\n        end\n        𝐂[m,n] = 𝐂ₘₙ\n    end\nend","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"and this can handle all transposed/not-tranposed permutations. LoopVectorization will change loop orders and strategy as appropriate based on the types of the input matrices. For each of the others, I wrote separate functions to handle each case.  Letting all three matrices be square and Size x Size, we attain the following benchmark results:","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"(Image: AmulB) This is classic GEMM, 𝐂 = 𝐀 * 𝐁. GFortran's intrinsic matmul function does fairly well. But all the compilers are well behind LoopVectorization here, which falls behind MKL's gemm beyond 70x70 or so. The problem imposed by alignment is also striking: performance is much higher when the sizes are integer multiplies of 8. Padding arrays so that each column is aligned regardless of the number of rows can thus be very profitable. PaddedMatrices.jl offers just such arrays in Julia. I believe that is also what the -pad compiler flag does when using Intel's compilers.","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"(Image: AmulBt) The optimal pattern for 𝐂 = 𝐀 * 𝐁ᵀ is almost identical to that for 𝐂 = 𝐀 * 𝐁. Yet, gfortran's matmul instrinsic stumbles, surprisingly doing much worse than gfortran + loops, and almost certainly worse than allocating memory for 𝐁ᵀ and creating the ecplicit copy.","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"ifort did equally well whethor or not 𝐁 was transposed, while LoopVectorization's performance degraded slightly faster as a function of size in the transposed case, because strides between memory accesses are larger when 𝐁 is transposed. But it still performed best of all the compiled loops over this size range, losing out to MKL and eventually OpenBLAS. icc interestingly does better when it is transposed.","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"GEMM is easiest when the matrix 𝐀 is not tranposed (assuming column-major memory layouts), because then you can sum up columns of 𝐀 to store into 𝐂. If 𝐀 were transposed, then we cannot efficiently load contiguous elements from 𝐀 that can best stored directly in 𝐂. So for 𝐂 = 𝐀ᵀ * 𝐁, contiguous vectors along the k-loop have to be reduced, adding some overhead. (Image: AtmulB) Packing is critical for performance here. LoopVectorization does not pack, therefore it is well behind MKL and OpenBLAS, which do. Eigen packs, but is poorly optimized for this CPU architecture.","category":"page"},{"location":"examples/matrix_multiplication/#","page":"Matrix Multiplication","title":"Matrix Multiplication","text":"When both 𝐀 and 𝐁 are transposed, we now have 𝐂 = 𝐀ᵀ * 𝐁ᵀ = (𝐁 * 𝐀)ᵀ. (Image: AtmulBt) Julia, Clang, and gfortran all struggled to vectorize this, because none of the matrices share a contiguous access: M for 𝐂, K for 𝐀ᵀ, and N for 𝐁ᵀ. However, LoopVectorization and all the specialized matrix multiplication functions managed to do about as well as normal; transposing while storing the results takes negligible amounts of time relative to the matrix multiplication itself. The ifort-loop version also did fairly well.","category":"page"},{"location":"examples/sum_of_squared_error/#Sum-of-squared-error-1","page":"Sum of squared error","title":"Sum of squared error","text":"","category":"section"},{"location":"examples/sum_of_squared_error/#","page":"Sum of squared error","title":"Sum of squared error","text":"To calculate (y - X * β)'(y - X * β), we can use the following loop.","category":"page"},{"location":"examples/sum_of_squared_error/#","page":"Sum of squared error","title":"Sum of squared error","text":"function sse_avx(y, X, β)\n    lp = zero(eltype(y))\n    @avx for i ∈ eachindex(y)\n        δ = y[i]\n        for j ∈ eachindex(β)\n            δ -= X[i,j] * β[j]\n        end\n        lp += δ * δ\n    end\n    lp\nend","category":"page"},{"location":"examples/sum_of_squared_error/#","page":"Sum of squared error","title":"Sum of squared error","text":"This example demonstrates the importance of (not) modeling memory bandwidth and cache, as the performance quickly drops dramatically. However, it still does much better than all the compiled loops, with only the BLAS gemv-based approach matching (and ultimately beating) it in performance, while the other compilers lagged well behind.","category":"page"},{"location":"examples/sum_of_squared_error/#","page":"Sum of squared error","title":"Sum of squared error","text":"Performance starts to degrade for sizes larger than 60. Letting N be the size, X was a 3N/2x N/2 matrix. Therefore, performance started to suffer when X had more than about 30 columns (performance is much less sensitive to the number of rows).","category":"page"},{"location":"examples/sum_of_squared_error/#","page":"Sum of squared error","title":"Sum of squared error","text":"(Image: sse)","category":"page"},{"location":"getting_started/#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To install LoopVectorization.jl, simply use the package and ] add LoopVectorization, or","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"LoopVectorization\")","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Currently LoopVectorization only supports rectangular iteration spaces, although I plan on extending it to triangular and ragged iteration spaces in the future. This means that if you nest multiple loops, the number of iterations of the inner loops shouldn't be a function of the outer loops. For example,","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using LoopVectorization \n\nfunction mvp(P, basis, coeffs::Vector{T}) where {T}\n    C = length(coeffs)\n    A = size(P, 1)\n    p = zero(T)\n    @avx for c ∈ 1:C\n        pc = coeffs[c]\n        for a = 1:A\n            pc *= P[a, basis[a, c]]\n        end\n        p += pc\n    end\n\tp\nend\n\nmaxdeg = 20; nbasis = 1_000; dim = 15;\nr = 1:maxdeg+1\nbasis = rand(r, (dim, nbasis));\ncoeffs = rand(T, nbasis);\nP = rand(T, dim, maxdeg+1);\n\nmvp(P, basis, coeffs)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Aside from loops, LoopVectorization.jl also supports broadcasting.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"danger: Danger\nBroadcasting an Array A when size(A,1) == 1 is NOT SUPPORTED, unless this is known at compile time (e.g., broadcasting a transposed vector is fine). Otherwise, you will probably crash Julia.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> using LoopVectorization, BenchmarkTools\n\njulia> M, K, N = 47, 73, 7;\n\njulia> A = rand(M, K);\n\njulia> b = rand(K);\n\njulia> c = rand(M);\n\njulia> d = rand(1,K,N);\n\njulia> #You can use a LowDimArray when you have a leading dimension of size 1.\n       ldad = LowDimArray{(false,true,true)}(d);\n\njulia> E1 = Array{Float64}(undef, M, K, N);\n\njulia> E2 = similar(E1);\n\njulia> @benchmark @. $E1 = exp($A - $b' +    $d) * $c\nBenchmarkTools.Trial: \n  memory estimate:  112 bytes\n  allocs estimate:  5\n  --------------\n  minimum time:     224.142 μs (0.00% GC)\n  median time:      225.773 μs (0.00% GC)\n  mean time:        229.146 μs (0.00% GC)\n  maximum time:     289.601 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> @benchmark @avx @. $E2 = exp($A - $b' + $ldad) * $c\nBenchmarkTools.Trial: \n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     19.666 μs (0.00% GC)\n  median time:      19.737 μs (0.00% GC)\n  mean time:        19.759 μs (0.00% GC)\n  maximum time:     29.906 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> E1 ≈ E2\ntrue","category":"page"},{"location":"examples/dot_product/#Dot-Products-1","page":"Dot Products","title":"Dot Products","text":"","category":"section"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"Dot products are simple the sum of the elementwise products of two vectors. They can be interpreted geometrically as (after normalizing by dividing by the norms of both vectors) yielding the cosine of the angle between them. This makes them useful for, for example, the No-U-Turn sampler to check for u-turns (i.e., to check if the current momentum is no longer in the same direction as the change in position).","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"function jdotavx(a, b)\n    s = zero(eltype(a))\n    @avx for i ∈ eachindex(a, b)\n        s += a[i] * b[i]\n    end\n    s\nend","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"To execute the loop using SIMD (Single Instruction Multiple Data) instructions, you have to unroll the loop. Rather than evaluating the loop as written – adding element-wise products to a single accumulator one after the other – you can multiply short vectors loaded from a and b and add their results to a vector of accumulators. ","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"Most modern CPUs found in laptops or desktops have the AVX instruction set, which allows them to operate on 256 bit vectors – meaning the vectors can hold 4 double precision (64 bit) floats. Some have the AVX512 instruction set, which increases the vector size to 512 bits, and also adds many new instructions that make vectorizing easier. To be gemeral across CPUs and data types, I'll refer to the number of elements in the vectors with W. I'll also refer to unrolling a loop by a factor of W and loading vectors from it as \"vectorizing\" that loop.","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"In addition to vectorizing the loop, we'll want to unroll it by an additional factor. Given that we have single or double precision floating point elements, most recent CPU cores have a potential throughput of two fused multiply-add (fma) instructions per clock cycle. However, it actually takes about four clock cycles for any of these instructions to execute; a single core is able to work on several in parallel.","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"This means that if we used a single vector to accumulate a product, we'd only get to perform one fused multiply add every four clock cycles: we'd have to wait for one instruction to complete before starting the next. By using extra accumulation vectors, we can break up this dependency chain. If we had 8 accumulators, then theoretically we could perform two per clock cycle, and after the 4th cycle, our first operations are done so that we can reuse them.","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"However, there is another bottle neck: we can only perform 2 aligned loads per clock cycle (or 1 unaligned load). [Alignment here means with respect to a memory address boundary, if your vectors are 256 bits, then a load/store is aligned if it is with respect to a memory address that is an integer multiple of 32 bytes (256 bits = 32 bytes).] Thus, in 4 clock cycles, we can do up to 8 loads. But each fma requires 2 loads, meaning we are limited to 4 of them per 4 clock cyles, and any unrolling beyond 4 gives us no benefit.","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"Double precision benchmarks pitting Julia's builtin dot product (named MKL here), and code compiled with a variety of compilers: (Image: dot) What we just described is the core of the approach used by all these compilers. The variation in results is explained mostly by how they handle vectors with lengths that are not an integer multiple of W. I ran these on a computer with AVX512 so that W = 8. LLVM, the backend compiler of both Julia and Clang, shows rapid performance degredation as N % 4W increases, where N is the length of the vectors. This is because, to handle the remainder, it uses a scalar loop that runs as written: multiply and add single elements, one after the other. ","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"GCC (gfortran) stumbles in throughput, because it does not use separate accumulation vectors.","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"The Intel compilers have a secondary vectorized loop without any additional unrolling that masks off excess lanes beyond N (for when N isn't an integer multiple of W). LoopVectorization uses if/ifelse checks to determine how many extra vectors are needed, the last of which is masked.","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"Neither GCC nor LLVM use masks (without LoopVectorization's assitance).","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"I am not certain, but I believe Intel and GCC check for the vector's alignment, and align them if neccessary. Julia guarantees that the start of arrays beyond a certain size are aligned, so this is not an optimization I have implemented. But it may be worthwhile for handling large matrices with a number of rows that isn't an integer multiple of W. For such matrices, the first column may be aligned, but the next will not be.","category":"page"},{"location":"examples/dot_product/#Dot-Self-1","page":"Dot Products","title":"Dot-Self","text":"","category":"section"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"A related problem is taking the dot product of a vector with itself; taking the sum of squares is a common operation, for example when calculating the (log)density of independent normal variates:","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"function jselfdotavx(a)\n    s = zero(eltype(a))\n    @avx for i ∈ eachindex(a)\n        s += a[i] * a[i]\n    end\n    s\nend","category":"page"},{"location":"examples/dot_product/#","page":"Dot Products","title":"Dot Products","text":"Because we only need a single load per fma-instruction, we can now benefit from having 8 separate accumulators. For this reason, LoopVectorization now unrolls by 8 – it decides how much to unroll by comparing the bottlenecks on throughput with latency. The other compilers do not change their behavior, so now LoopVectorization has the advantage: (Image: selfdot) This algorithm may need refinement, because Julia (without LoopVectorization) only unrolls by 4, yet achieves roughly the same performance as LoopVectorization at multiples of 4W = 32, although performance declines rapidly from there due to the slow scalar loop. Performance for most is much higher – more GFLOPS – than the normal dot product, but still under half of the CPU's potential 131.2 GFLOPS, suggesting that some other bottlenecks are preventing the core from attaining 2 fmas per clock cycle. Note also that 8W = 64, so we don't really have enough iterations of the loop to amortize the overhead of performing the reductions of all these vectors into a single scalar. By the time the vectors are long enough to do this, we'll start running into memory bandwidth bottlenecks.","category":"page"},{"location":"devdocs/loopset_structure/#LoopSet-Structure-1","page":"LoopSet Structure","title":"LoopSet Structure","text":"","category":"section"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"The loopsets define loops as a set of operations that depend on one another, and also on loops. Cycles are not allowed, making it a directed acyclic graph. Let's use a set of nested loops performing matrix multiplication as an example. We can create a naive LoopSet from an expression (naive due to being created without access to any type information):","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> using LoopVectorization\n\njulia> AmulBq = :(for m ∈ 1:M, n ∈ 1:N\n           C[m,n] = zero(eltype(B))\n           for k ∈ 1:K\n               C[m,n] += A[m,k] * B[k,n]\n           end\n       end);\n\njulia> lsAmulB = LoopVectorization.LoopSet(AmulBq);","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"This LoopSet consists of seven operations that define the relationships within the loop:","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)\n7-element Array{LoopVectorization.Operation,1}:\n var\"##RHS#256\" = var\"##zero#257\"\n C[m, n] = var\"##RHS#256\"\n var\"##tempload#258\" = A[m, k]\n var\"##tempload#259\" = B[k, n]\n var\"##reduction#260\" = var\"##reductzero#261\"\n var\"##reduction#260\" = LoopVectorization.vfmadd_fast(var\"##tempload#258\", var\"##tempload#259\", var\"##reduction#260\")\n var\"##RHS#256\" = LoopVectorization.reduce_to_add(var\"##reduction#260\", var\"##RHS#256\")","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"The act of performing a \"reduction\" across a loop introduces a few extra operations that manage creating a \"zero\" with respect to the reduction, and then combining with the specified value using reduce_to_add, which performs any necessary type conversions, such as from an SVec vector-type to a scalar, if necessary. This simplifies code generation, by making the functions agnostic with respect to the actual vectorization decisions the library makes.","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"Each operation is listed as depending on a set of loop iteration symbols:","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.loopdependencies.(LoopVectorization.operations(lsAmulB))\n7-element Array{Array{Symbol,1},1}:\n [:m, :n]\n [:m, :n]\n [:m, :k]\n [:k, :n]\n [:m, :n]\n [:m, :k, :n]\n [:m, :n]","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"We can also see which of the operations each of these operations depend on:","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)[6]\nvar\"##reduction#260\" = LoopVectorization.vfmadd_fast(var\"##tempload#258\", var\"##tempload#259\", var\"##reduction#260\")\n\njulia> LoopVectorization.parents(ans)\n3-element Array{LoopVectorization.Operation,1}:\n var\"##tempload#258\" = A[m, k]\n var\"##tempload#259\" = B[k, n]\n var\"##reduction#260\" = var\"##reductzero#261\"","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"References to arrays are represented with an ArrayReferenceMeta data structure:","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"julia> LoopVectorization.operations(lsAmulB)[3].ref\nLoopVectorization.ArrayReferenceMeta(LoopVectorization.ArrayReference(:A, [:m, :k], Int8[0, 0]), Bool[1, 1], Symbol(\"##vptr##_A\"))","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"It contains the name of the parent array (:A), the indicies [:m,:k], and a boolean vector (Bool[1, 1]) indicating whether these indices are loop iterables. Note that the optimizer assumes arrays are column-major, and thus that it is efficient to read contiguous elements from the first index. In lower level terms, it means that high-throughput vmov instructions can be used rather than low-throughput gathers. Similar story for storing elements. When no axis has unit stride, the first given index will be the dummy Symbol(\"##DISCONTIGUOUSSUBARRAY##\").","category":"page"},{"location":"devdocs/loopset_structure/#","page":"LoopSet Structure","title":"LoopSet Structure","text":"warning: Warning\nCurrently, only single return values are supported (tuple destructuring is not supported in assignments).","category":"page"},{"location":"examples/filtering/#Image-Filtering-1","page":"Image Filtering","title":"Image Filtering","text":"","category":"section"},{"location":"examples/filtering/#","page":"Image Filtering","title":"Image Filtering","text":"Here, we convolve a small matrix kern with a larger matrix A, storing the results in out, using Julia's generic Cartesian Indexing:","category":"page"},{"location":"examples/filtering/#","page":"Image Filtering","title":"Image Filtering","text":"using LoopVectorization, OffsetArrays, Images\nkern = Images.Kernel.gaussian((1, 1), (3, 3))\nfunction filter2davx!(out::AbstractMatrix, A::AbstractMatrix, kern)\n    @avx for J in CartesianIndices(out)\n        tmp = zero(eltype(out))\n        for I ∈ CartesianIndices(kern)\n            tmp += A[I + J] * kern[I]\n        end\n        out[J] = tmp\n    end\n    out\nend","category":"page"},{"location":"examples/filtering/#","page":"Image Filtering","title":"Image Filtering","text":"These are effectively four nested loops. For all the benchmarks, kern was 3 by 3, making it too small for vectorizing these loops to be particularly profitable. By vectorizing an outer loop instead, it can benefit from SIMD and also avoid having to do a reduction (horizontal addition) of a vector before storing in out, as the vectors can then be stored directly. (Image: dynamicfilter)","category":"page"},{"location":"examples/filtering/#","page":"Image Filtering","title":"Image Filtering","text":"LoopVectorization achieved much better performance than all the alternatives, which tried vectorizing the inner loops. By making the compilers aware that the inner loops are too short to be worth vectorizing, we can get them to vectorize an outer loop instead. By defining the size of kern as constant in C and Fortran, and using size parameters in Julia, we can inform the compilers: (Image: staticsizefilter) Now all are doing much better than they were before, although still well shy of the 131.2 GFLOPS theoretical limit for the host CPU cores. While they all improved, two are lagging behind the main group:","category":"page"},{"location":"examples/filtering/#","page":"Image Filtering","title":"Image Filtering","text":"ifort lags behind all the others except base Julia. I'll need to do more investigating to find out why.\nBase Julia. While providing static size information was enough for it to realize vectorizing the inner loops was not worth it, base Julia was seemingly the only one that didn't decide to vectorize an outer loop instead.","category":"page"},{"location":"examples/filtering/#","page":"Image Filtering","title":"Image Filtering","text":"Manually unrolling the inner loops allows base Julia to vectorize, while the performance of all non-Julia variants was unchanged: (Image: unrolledfilter) LoopVectorization is currently limited to only unrolling two loops (but a third may be vectorized, effectively unrolling it by the length of the vectors). Manually unrolling two of the loops lets up to four loops be unrolled.","category":"page"},{"location":"#LoopVectorization.jl-1","page":"Home","title":"LoopVectorization.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This documentation is for LoopVectorization.jl. Please file an issue if you run into any problems.","category":"page"},{"location":"#Manual-Outline-1","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n    \"getting_started.md\",\n    \"examples/matrix_multiplication.md\",\n    \"examples/matrix_vector_ops.md\",\n    \"examples/dot_product.md\",\n    \"examples/filtering.md\",\n    \"examples/special_functions.md\",\n    \"examples/sum_of_squared_error.md\",\n    \"vectorized_convenience_functions.md\",\n    \"future_work.md\",\n\t\"devdocs/overview.md\",\n\t\"devdocs/loopset_structure.md\",\n\t\"devdocs/constructing_loopsets.md\",\n\t\"devdocs/evaluating_loops.md\",\n\t\"devdocs/lowering.md\"\n]\nDepth = 1","category":"page"}]
}
